{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAGAS Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "documentation: https://docs.ragas.io/en/stable/getstarted/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it gives Attribute Error LlamaForCausalLM' object has no attribute 'set_run_config'  during evaluation\n",
    "\n",
    "Tested also using LangchainLLMWrapper. No good results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\app\\BaseToolkit\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [02:35<00:00, 38.97s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# embedding model\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\") #they used \"BAAI/bge-m3\"\n",
    "\n",
    "# evaluator\n",
    "model_id = \"swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_samples = {\n",
    "    'question': ['When was the first super bowl?', 'Who won the most super bowls?'],\n",
    "    'answer': ['The first superbowl was held on Jan 15, 1967', 'The most super bowls have been won by The New England Patriots'],\n",
    "    'contexts' : [['The First AFL–NFL World Championship Game was an American football game played on January 15, 1967, at the Los Angeles Memorial Coliseum in Los Angeles,'], \n",
    "                    ['The Green Bay Packers...Green Bay, Wisconsin.','The Packers compete...Football Conference']],\n",
    "    'ground_truth': ['The first superbowl was held on January 15, 1967', 'The New England Patriots have won the Super Bowl a record six times']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset \n",
    "\n",
    "dataset = Dataset.from_dict(data_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")\n",
    "\n",
    "metrics=[\n",
    "        context_precision,\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "        context_recall,\n",
    "    ]\n",
    "\n",
    "#metrics = [\n",
    " #   BLEU(), ROUGE(), METEOR(), BertScore(), NIST(), CIDEr(), TER(), Spice() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LlamaForCausalLM' object has no attribute 'set_run_config'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m evaluate\n\u001b[1;32m----> 2\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_llm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\app\\BaseToolkit\\lib\\site-packages\\ragas\\evaluation.py:198\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(dataset, metrics, llm, embeddings, callbacks, in_ci, is_async, run_config, raise_exceptions, column_map)\u001b[0m\n\u001b[0;32m    195\u001b[0m             reproducable_metrics\u001b[38;5;241m.\u001b[39mappend(i)\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;66;03m# init all the models\u001b[39;00m\n\u001b[1;32m--> 198\u001b[0m     \u001b[43mmetric\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    200\u001b[0m executor \u001b[38;5;241m=\u001b[39m Executor(\n\u001b[0;32m    201\u001b[0m     desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    202\u001b[0m     keep_progress_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    203\u001b[0m     raise_exceptions\u001b[38;5;241m=\u001b[39mraise_exceptions,\n\u001b[0;32m    204\u001b[0m     run_config\u001b[38;5;241m=\u001b[39mrun_config,\n\u001b[0;32m    205\u001b[0m )\n\u001b[0;32m    206\u001b[0m \u001b[38;5;66;03m# new evaluation chain\u001b[39;00m\n",
      "File \u001b[1;32mc:\\app\\BaseToolkit\\lib\\site-packages\\ragas\\metrics\\base.py:159\u001b[0m, in \u001b[0;36mMetricWithLLM.init\u001b[1;34m(self, run_config)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMetric \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has no valid LLM provided (self.llm is None). Please initantiate a the metric with an LLM to run.\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     )\n\u001b[1;32m--> 159\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_run_config\u001b[49m(run_config)\n",
      "File \u001b[1;32mc:\\app\\BaseToolkit\\lib\\site-packages\\torch\\nn\\modules\\module.py:1688\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1687\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1688\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'LlamaForCausalLM' object has no attribute 'set_run_config'"
     ]
    }
   ],
   "source": [
    "from ragas import evaluate\n",
    "score = evaluate(dataset, metrics=metrics, llm=model, embeddings=embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving standard way using LangchainLLMWrapper and LangchainEmbeddingsWrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attach to the existing event loop when using jupyter notebooks\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\app\\BaseToolkit\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [01:09<00:00, 34.82s/it]\n",
      "c:\\app\\BaseToolkit\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 0.3. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFacePipeline`.\n",
      "  warn_deprecated(\n",
      "c:\\app\\BaseToolkit\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")\n",
    "from ragas import evaluate\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"#\"swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "pipe = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=True,\n",
    "    task='text-generation',\n",
    "    temperature=0.1, \n",
    "    repetition_penalty=1.1,\n",
    "    max_new_tokens=2000\n",
    ")\n",
    "\n",
    "\n",
    "langchain_llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n",
    "\n",
    "# Wrap the LLM and embeddings with the RAGAS wrappers\n",
    "langchain_llm = LangchainLLMWrapper(langchain_llm)\n",
    "langchain_embeddings = LangchainEmbeddingsWrapper(embedding_model)\n",
    "\n",
    "\n",
    "data_samples = {\n",
    "    'question': ['When was the first super bowl?', 'Who won the most super bowls?'],\n",
    "    'answer': ['The first superbowl was held on Jan 15, 1967', 'The most super bowls have been won by The New England Patriots'],\n",
    "    'contexts': [['The First AFL–NFL World Championship Game was an American football game played on January 15, 1967, at the Los Angeles Memorial Coliseum in Los Angeles,'],\n",
    "                 ['The Green Bay Packers...Green Bay, Wisconsin.', 'The Packers compete...Football Conference']],\n",
    "    'ground_truth': ['The first superbowl was held on January 15, 1967', 'The New England Patriots have won the Super Bowl a record six times']\n",
    "}\n",
    "\n",
    "\n",
    "dataset = Dataset.from_dict(data_samples)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_samples = {\n",
    "    'question': ['When was the first super bowl?', 'Who won the most super bowls?'],\n",
    "    'answer': ['The first superbowl was held on Jan 15, 1967', 'The most super bowls have been won by The New England Patriots'],\n",
    "    'contexts': [['The First AFL–NFL World Championship Game was an American football game played on January 15, 1967, at the Los Angeles Memorial Coliseum in Los Angeles,'],\n",
    "                 ['The Green Bay Packers...Green Bay, Wisconsin.', 'The Packers compete...Football Conference']],\n",
    "    'ground_truth': ['The first superbowl was held on January 15, 1967', 'The New England Patriots have won the Super Bowl a record six times']\n",
    "}\n",
    "\n",
    "\n",
    "dataset = Dataset.from_dict(data_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ragas\n",
    "run_config = ragas.RunConfig(timeout=10, max_retries=1, max_wait=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\app\\BaseToolkit\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n",
      "Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\asyncio\\tasks.py\", line 256, in __step\n",
      "    result = coro.send(None)\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\site-packages\\ragas\\metrics\\_context_precision.py\", line 161, in _ascore\n",
      "    results = await self.llm.generate(\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\site-packages\\ragas\\llms\\base.py\", line 93, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\site-packages\\tenacity\\_asyncio.py\", line 88, in async_wrapped\n",
      "    return await fn(*args, **kwargs)\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\site-packages\\tenacity\\_asyncio.py\", line 47, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\site-packages\\tenacity\\__init__.py\", line 314, in iter\n",
      "    return fut.result()\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\concurrent\\futures\\_base.py\", line 439, in result\n",
      "    return self.__get_result()\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\concurrent\\futures\\_base.py\", line 391, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\site-packages\\tenacity\\_asyncio.py\", line 50, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\site-packages\\ragas\\llms\\base.py\", line 178, in agenerate_text\n",
      "    result = await self.langchain_llm.agenerate_prompt(\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\site-packages\\langchain_core\\language_models\\llms.py\", line 643, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\site-packages\\langchain_core\\language_models\\llms.py\", line 1018, in agenerate\n",
      "    output = await self._agenerate_helper(\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\site-packages\\langchain_core\\language_models\\llms.py\", line 882, in _agenerate_helper\n",
      "    raise e\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\site-packages\\langchain_core\\language_models\\llms.py\", line 866, in _agenerate_helper\n",
      "    await self._agenerate(\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\site-packages\\langchain_core\\language_models\\llms.py\", line 544, in _agenerate\n",
      "    return await run_in_executor(\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\site-packages\\langchain_core\\runnables\\config.py\", line 557, in run_in_executor\n",
      "    return await asyncio.get_running_loop().run_in_executor(\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\asyncio\\futures.py\", line 284, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\asyncio\\tasks.py\", line 328, in __wakeup\n",
      "    future.result()\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\asyncio\\futures.py\", line 196, in result\n",
      "    raise exc\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\asyncio\\tasks.py\", line 490, in wait_for\n",
      "    return fut.result()\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\asyncio\\futures.py\", line 196, in result\n",
      "    raise exc\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\site-packages\\ragas\\executor.py\", line 104, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\site-packages\\ragas\\metrics\\base.py\", line 134, in ascore\n",
      "    raise e\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\site-packages\\ragas\\metrics\\base.py\", line 127, in ascore\n",
      "    score = await asyncio.wait_for(\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\asyncio\\tasks.py\", line 492, in wait_for\n",
      "    raise exceptions.TimeoutError() from exc\n",
      "asyncio.exceptions.TimeoutError\n",
      "Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\asyncio\\tasks.py\", line 256, in __step\n",
      "    result = coro.send(None)\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\site-packages\\ragas\\metrics\\_context_precision.py\", line 161, in _ascore\n",
      "    results = await self.llm.generate(\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\site-packages\\ragas\\llms\\base.py\", line 93, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\site-packages\\tenacity\\_asyncio.py\", line 88, in async_wrapped\n",
      "    return await fn(*args, **kwargs)\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\site-packages\\tenacity\\_asyncio.py\", line 47, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\site-packages\\tenacity\\__init__.py\", line 314, in iter\n",
      "    return fut.result()\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\concurrent\\futures\\_base.py\", line 439, in result\n",
      "    return self.__get_result()\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\concurrent\\futures\\_base.py\", line 391, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\site-packages\\tenacity\\_asyncio.py\", line 50, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\site-packages\\ragas\\llms\\base.py\", line 178, in agenerate_text\n",
      "    result = await self.langchain_llm.agenerate_prompt(\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\site-packages\\langchain_core\\language_models\\llms.py\", line 643, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\site-packages\\langchain_core\\language_models\\llms.py\", line 1018, in agenerate\n",
      "    output = await self._agenerate_helper(\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\site-packages\\langchain_core\\language_models\\llms.py\", line 882, in _agenerate_helper\n",
      "    raise e\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\site-packages\\langchain_core\\language_models\\llms.py\", line 866, in _agenerate_helper\n",
      "    await self._agenerate(\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\site-packages\\langchain_core\\language_models\\llms.py\", line 544, in _agenerate\n",
      "    return await run_in_executor(\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\site-packages\\langchain_core\\runnables\\config.py\", line 557, in run_in_executor\n",
      "    return await asyncio.get_running_loop().run_in_executor(\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\asyncio\\futures.py\", line 284, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\asyncio\\tasks.py\", line 328, in __wakeup\n",
      "    future.result()\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\asyncio\\futures.py\", line 196, in result\n",
      "    raise exc\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\asyncio\\tasks.py\", line 490, in wait_for\n",
      "    return fut.result()\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\asyncio\\futures.py\", line 196, in result\n",
      "    raise exc\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\site-packages\\ragas\\executor.py\", line 104, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\site-packages\\ragas\\metrics\\base.py\", line 134, in ascore\n",
      "    raise e\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\site-packages\\ragas\\metrics\\base.py\", line 127, in ascore\n",
      "    score = await asyncio.wait_for(\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\asyncio\\tasks.py\", line 492, in wait_for\n",
      "    raise exceptions.TimeoutError() from exc\n",
      "asyncio.exceptions.TimeoutError\n",
      "Evaluating: 100%|██████████| 2/2 [01:20<00:00, 40.02s/it]\n",
      "c:\\app\\BaseToolkit\\lib\\site-packages\\ragas\\evaluation.py:304: RuntimeWarning: Mean of empty slice\n",
      "  value = np.nanmean(self.scores[cn])\n"
     ]
    }
   ],
   "source": [
    "result = evaluate(\n",
    "    dataset=dataset,\n",
    "    llm=langchain_llm,\n",
    "    embeddings=langchain_embeddings,\n",
    "    metrics=[\n",
    "        context_precision#,\n",
    "        #faithfulness#,\n",
    "        #answer_relevancy#,\n",
    "        #context_recall,\n",
    "    ],\n",
    "    raise_exceptions=False,\n",
    "    run_config= run_config # Ensure exceptions are raised directly\n",
    "    #is_async = True  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context_precision': nan}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ollama case (no Lllamantino)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attach to the existing event loop when using jupyter notebooks\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.chat_models import ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain_embeddings = OllamaEmbeddings(model = \"llama3\")\n",
    "langchain_llm = ChatOllama(model = \"llama3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\app\\BaseToolkit\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset \n",
    "data_samples = {\n",
    "    'question': ['When was the first super bowl?'],#, 'Who won the most super bowls?'],\n",
    "    'answer': ['The first superbowl was held on Jan 15, 1967'],# 'The most super bowls have been won by The New England Patriots'],\n",
    "    'contexts' : [['The First AFL–NFL World Championship Game was an American football game played on January 15, 1967, at the Los Angeles Memorial Coliseum in Los Angeles,']], \n",
    "    #['The Green Bay Packers...Green Bay, Wisconsin.','The Packers compete...Football Conference']],\n",
    "    'ground_truth': ['The first superbowl was held on January 15, 1967']#, 'The New England Patriots have won the Super Bowl a record six times']\n",
    "}\n",
    "dataset = Dataset.from_dict(data_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")\n",
    "\n",
    "metrics=[\n",
    "        context_precision,\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "        context_recall,\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/4 [01:00<?, ?it/s]\n",
      "Exception in thread Thread-37:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\asyncio\\tasks.py\", line 258, in __step\n",
      "    result = coro.throw(exc)\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\site-packages\\ragas\\metrics\\_context_recall.py\", line 169, in _ascore\n",
      "    results = await self.llm.generate(\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\site-packages\\ragas\\llms\\base.py\", line 93, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\site-packages\\tenacity\\_asyncio.py\", line 88, in async_wrapped\n",
      "    return await fn(*args, **kwargs)\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\site-packages\\tenacity\\_asyncio.py\", line 57, in __call__\n",
      "    await self.sleep(do)\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\asyncio\\tasks.py\", line 652, in sleep\n",
      "    return await future\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\asyncio\\futures.py\", line 284, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\asyncio\\tasks.py\", line 328, in __wakeup\n",
      "    future.result()\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\asyncio\\futures.py\", line 196, in result\n",
      "    raise exc\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\asyncio\\tasks.py\", line 490, in wait_for\n",
      "    return fut.result()\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\asyncio\\futures.py\", line 196, in result\n",
      "    raise exc\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\threading.py\", line 980, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\site-packages\\ragas\\executor.py\", line 87, in run\n",
      "    results = self.loop.run_until_complete(self._aresults())\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\asyncio\\futures.py\", line 201, in result\n",
      "    raise self._exception\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\asyncio\\tasks.py\", line 256, in __step\n",
      "    result = coro.send(None)\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\site-packages\\ragas\\executor.py\", line 79, in _aresults\n",
      "    r = await future\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\asyncio\\tasks.py\", line 611, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\asyncio\\futures.py\", line 201, in result\n",
      "    raise self._exception\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\asyncio\\tasks.py\", line 256, in __step\n",
      "    result = coro.send(None)\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\site-packages\\ragas\\executor.py\", line 37, in sema_coro\n",
      "    return await coro\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\site-packages\\ragas\\executor.py\", line 109, in wrapped_callable_async\n",
      "    raise e\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\site-packages\\ragas\\executor.py\", line 104, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\site-packages\\ragas\\metrics\\base.py\", line 134, in ascore\n",
      "    raise e\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\site-packages\\ragas\\metrics\\base.py\", line 127, in ascore\n",
      "    score = await asyncio.wait_for(\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\asyncio\\tasks.py\", line 492, in wait_for\n",
      "    raise exceptions.TimeoutError() from exc\n",
      "asyncio.exceptions.TimeoutError\n"
     ]
    },
    {
     "ename": "ExceptionInRunner",
     "evalue": "The runner thread which was running the jobs raised an exeception. Read the traceback above to debug it. You can also pass `raise_exceptions=False` incase you want to show only a warning message instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mExceptionInRunner\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01masyncio\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RunConfig\n\u001b[0;32m      3\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mwait_for(\n\u001b[1;32m----> 4\u001b[0m                     evaluate(dataset, metrics\u001b[38;5;241m=\u001b[39mmetrics, llm \u001b[38;5;241m=\u001b[39m langchain_llm, embeddings \u001b[38;5;241m=\u001b[39m langchain_embeddings, run_config\u001b[38;5;241m=\u001b[39mRunConfig(thread_timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m60\u001b[39m)),\n\u001b[0;32m      5\u001b[0m                     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m600\u001b[39m  \u001b[38;5;66;03m# 10 minutes timeout \u001b[39;00m\n\u001b[0;32m      6\u001b[0m                     )\n",
      "File \u001b[1;32mc:\\app\\BaseToolkit\\lib\\site-packages\\ragas\\evaluation.py:255\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(dataset, metrics, llm, embeddings, callbacks, in_ci, is_async, run_config, raise_exceptions, column_map)\u001b[0m\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evaluation_group_cm\u001b[38;5;241m.\u001b[39mended:\n\u001b[0;32m    253\u001b[0m         evaluation_rm\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 255\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     result \u001b[38;5;241m=\u001b[39m Result(\n\u001b[0;32m    258\u001b[0m         scores\u001b[38;5;241m=\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_list(scores),\n\u001b[0;32m    259\u001b[0m         dataset\u001b[38;5;241m=\u001b[39mdataset,\n\u001b[0;32m    260\u001b[0m         binary_columns\u001b[38;5;241m=\u001b[39mbinary_metrics,\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\app\\BaseToolkit\\lib\\site-packages\\ragas\\evaluation.py:237\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(dataset, metrics, llm, embeddings, callbacks, in_ci, is_async, run_config, raise_exceptions, column_map)\u001b[0m\n\u001b[0;32m    235\u001b[0m results \u001b[38;5;241m=\u001b[39m executor\u001b[38;5;241m.\u001b[39mresults()\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m results \u001b[38;5;241m==\u001b[39m []:\n\u001b[1;32m--> 237\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ExceptionInRunner()\n\u001b[0;32m    239\u001b[0m \u001b[38;5;66;03m# convert results to dataset_like\u001b[39;00m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataset):\n",
      "\u001b[1;31mExceptionInRunner\u001b[0m: The runner thread which was running the jobs raised an exeception. Read the traceback above to debug it. You can also pass `raise_exceptions=False` incase you want to show only a warning message instead."
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from ragas.llms.base import RunConfig\n",
    "result = await asyncio.wait_for(\n",
    "                    evaluate(dataset, metrics=metrics, llm = langchain_llm, embeddings = langchain_embeddings, run_config=RunConfig(thread_timeout=60)),\n",
    "                    timeout=600  # 10 minutes timeout \n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/4 [01:20<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TimeoutError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\app\\BaseToolkit\\lib\\asyncio\\tasks.py:258\u001b[0m, in \u001b[0;36mTask.__step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    257\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 258\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\app\\BaseToolkit\\lib\\site-packages\\ragas\\metrics\\_context_recall.py:168\u001b[0m, in \u001b[0;36mContextRecall._ascore\u001b[1;34m(self, row, callbacks)\u001b[0m\n\u001b[0;32m    167\u001b[0m p_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_context_recall_prompt(row)\n\u001b[1;32m--> 168\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m    169\u001b[0m     p_value,\n\u001b[0;32m    170\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m    171\u001b[0m     n\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreproducibility,\n\u001b[0;32m    172\u001b[0m )\n\u001b[0;32m    173\u001b[0m results \u001b[38;5;241m=\u001b[39m [results\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][i]\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreproducibility)]\n",
      "File \u001b[1;32mc:\\app\\BaseToolkit\\lib\\site-packages\\ragas\\llms\\base.py:95\u001b[0m, in \u001b[0;36mBaseRagasLLM.generate\u001b[1;34m(self, prompt, n, temperature, stop, callbacks, is_async)\u001b[0m\n\u001b[0;32m     92\u001b[0m     agenerate_text_with_retry \u001b[38;5;241m=\u001b[39m add_async_retry(\n\u001b[0;32m     93\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magenerate_text, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_config\n\u001b[0;32m     94\u001b[0m     )\n\u001b[1;32m---> 95\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m agenerate_text_with_retry(\n\u001b[0;32m     96\u001b[0m         prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[0;32m     97\u001b[0m         n\u001b[38;5;241m=\u001b[39mn,\n\u001b[0;32m     98\u001b[0m         temperature\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[0;32m     99\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    100\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m    101\u001b[0m     )\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\app\\BaseToolkit\\lib\\site-packages\\tenacity\\_asyncio.py:88\u001b[0m, in \u001b[0;36mAsyncRetrying.wraps.<locals>.async_wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21masync_wrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs: t\u001b[38;5;241m.\u001b[39mAny, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: t\u001b[38;5;241m.\u001b[39mAny) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mAny:\n\u001b[1;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\app\\BaseToolkit\\lib\\site-packages\\tenacity\\_asyncio.py:57\u001b[0m, in \u001b[0;36mAsyncRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     56\u001b[0m     retry_state\u001b[38;5;241m.\u001b[39mprepare_for_next_attempt()\n\u001b[1;32m---> 57\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msleep(do)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\app\\BaseToolkit\\lib\\asyncio\\tasks.py:652\u001b[0m, in \u001b[0;36msleep\u001b[1;34m(delay, result, loop)\u001b[0m\n\u001b[0;32m    651\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 652\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m future\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\app\\BaseToolkit\\lib\\asyncio\\futures.py:284\u001b[0m, in \u001b[0;36mFuture.__await__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    283\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_asyncio_future_blocking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 284\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m  \u001b[38;5;66;03m# This tells Task to wait for completion.\u001b[39;00m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[1;32mc:\\app\\BaseToolkit\\lib\\asyncio\\tasks.py:328\u001b[0m, in \u001b[0;36mTask.__wakeup\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 328\u001b[0m     \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\app\\BaseToolkit\\lib\\asyncio\\futures.py:196\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    195\u001b[0m     exc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_cancelled_error()\n\u001b[1;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m!=\u001b[39m _FINISHED:\n",
      "\u001b[1;31mCancelledError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\app\\BaseToolkit\\lib\\asyncio\\tasks.py:490\u001b[0m, in \u001b[0;36mwait_for\u001b[1;34m(fut, timeout, loop)\u001b[0m\n\u001b[0;32m    489\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 490\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mCancelledError \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\app\\BaseToolkit\\lib\\asyncio\\futures.py:196\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    195\u001b[0m     exc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_cancelled_error()\n\u001b[1;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m!=\u001b[39m _FINISHED:\n",
      "\u001b[1;31mCancelledError\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlangchain_llm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlangchain_embeddings\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\app\\BaseToolkit\\lib\\site-packages\\ragas\\evaluation.py:247\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(dataset, metrics, llm, embeddings, callbacks, in_ci, run_config, raise_exceptions, column_map)\u001b[0m\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evaluation_group_cm\u001b[38;5;241m.\u001b[39mended:\n\u001b[0;32m    245\u001b[0m         evaluation_rm\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 247\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    249\u001b[0m     result \u001b[38;5;241m=\u001b[39m Result(\n\u001b[0;32m    250\u001b[0m         scores\u001b[38;5;241m=\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_list(scores),\n\u001b[0;32m    251\u001b[0m         dataset\u001b[38;5;241m=\u001b[39mdataset,\n\u001b[0;32m    252\u001b[0m         binary_columns\u001b[38;5;241m=\u001b[39mbinary_metrics,\n\u001b[0;32m    253\u001b[0m     )\n",
      "File \u001b[1;32mc:\\app\\BaseToolkit\\lib\\site-packages\\ragas\\evaluation.py:227\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(dataset, metrics, llm, embeddings, callbacks, in_ci, run_config, raise_exceptions, column_map)\u001b[0m\n\u001b[0;32m    224\u001b[0m scores \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    225\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    226\u001b[0m     \u001b[38;5;66;03m# get the results\u001b[39;00m\n\u001b[1;32m--> 227\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mexecutor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresults\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m results \u001b[38;5;241m==\u001b[39m []:\n\u001b[0;32m    229\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ExceptionInRunner()\n",
      "File \u001b[1;32mc:\\app\\BaseToolkit\\lib\\site-packages\\ragas\\executor.py:107\u001b[0m, in \u001b[0;36mExecutor.results\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    103\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(r)\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 107\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_aresults\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m sorted_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(results, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [r[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m sorted_results]\n",
      "File \u001b[1;32mc:\\app\\BaseToolkit\\lib\\site-packages\\nest_asyncio.py:30\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[1;34m(main, debug)\u001b[0m\n\u001b[0;32m     28\u001b[0m task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(main)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[1;32mc:\\app\\BaseToolkit\\lib\\site-packages\\nest_asyncio.py:98\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m     97\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent loop stopped before Future completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\app\\BaseToolkit\\lib\\asyncio\\futures.py:201\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[1;32mc:\\app\\BaseToolkit\\lib\\asyncio\\tasks.py:256\u001b[0m, in \u001b[0;36mTask.__step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    253\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[0;32m    255\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[1;32m--> 256\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    257\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    258\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "File \u001b[1;32mc:\\app\\BaseToolkit\\lib\\site-packages\\ragas\\executor.py:102\u001b[0m, in \u001b[0;36mExecutor.results.<locals>._aresults\u001b[1;34m()\u001b[0m\n\u001b[0;32m     94\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m tqdm(\n\u001b[0;32m     96\u001b[0m     futures_as_they_finish,\n\u001b[0;32m     97\u001b[0m     desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdesc,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    100\u001b[0m     leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_progress_bar,\n\u001b[0;32m    101\u001b[0m ):\n\u001b[1;32m--> 102\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m future\n\u001b[0;32m    103\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(r)\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[1;32mc:\\app\\BaseToolkit\\lib\\asyncio\\tasks.py:611\u001b[0m, in \u001b[0;36mas_completed.<locals>._wait_for_one\u001b[1;34m()\u001b[0m\n\u001b[0;32m    608\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    609\u001b[0m     \u001b[38;5;66;03m# Dummy value from _on_timeout().\u001b[39;00m\n\u001b[0;32m    610\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTimeoutError\n\u001b[1;32m--> 611\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\app\\BaseToolkit\\lib\\asyncio\\futures.py:201\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[1;32mc:\\app\\BaseToolkit\\lib\\asyncio\\tasks.py:256\u001b[0m, in \u001b[0;36mTask.__step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    253\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[0;32m    255\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[1;32m--> 256\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    257\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    258\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "File \u001b[1;32mc:\\app\\BaseToolkit\\lib\\site-packages\\ragas\\executor.py:34\u001b[0m, in \u001b[0;36mas_completed.<locals>.sema_coro\u001b[1;34m(coro)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msema_coro\u001b[39m(coro):\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m semaphore:\n\u001b[1;32m---> 34\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m coro\n",
      "File \u001b[1;32mc:\\app\\BaseToolkit\\lib\\site-packages\\ragas\\executor.py:59\u001b[0m, in \u001b[0;36mExecutor.wrap_callable_with_index.<locals>.wrapped_callable_async\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraise_exceptions:\n\u001b[1;32m---> 59\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     61\u001b[0m         logger\u001b[38;5;241m.\u001b[39merror(\n\u001b[0;32m     62\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunner in Executor raised an exception\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     63\u001b[0m         )\n",
      "File \u001b[1;32mc:\\app\\BaseToolkit\\lib\\site-packages\\ragas\\executor.py:53\u001b[0m, in \u001b[0;36mExecutor.wrap_callable_with_index.<locals>.wrapped_callable_async\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     51\u001b[0m result \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnan\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 53\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m MaxRetriesExceeded \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;66;03m# this only for testset generation v2\u001b[39;00m\n\u001b[0;32m     56\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax retries exceeded for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39mevolution\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\app\\BaseToolkit\\lib\\site-packages\\ragas\\metrics\\base.py:127\u001b[0m, in \u001b[0;36mMetric.ascore\u001b[1;34m(self, row, callbacks, thread_timeout)\u001b[0m\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m group_cm\u001b[38;5;241m.\u001b[39mended:\n\u001b[0;32m    126\u001b[0m         rm\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 127\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m group_cm\u001b[38;5;241m.\u001b[39mended:\n",
      "File \u001b[1;32mc:\\app\\BaseToolkit\\lib\\site-packages\\ragas\\metrics\\base.py:120\u001b[0m, in \u001b[0;36mMetric.ascore\u001b[1;34m(self, row, callbacks, thread_timeout)\u001b[0m\n\u001b[0;32m    118\u001b[0m rm, group_cm \u001b[38;5;241m=\u001b[39m new_group(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, inputs\u001b[38;5;241m=\u001b[39mrow, callbacks\u001b[38;5;241m=\u001b[39mcallbacks)\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 120\u001b[0m     score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mwait_for(\n\u001b[0;32m    121\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ascore(row\u001b[38;5;241m=\u001b[39mrow, callbacks\u001b[38;5;241m=\u001b[39mgroup_cm),\n\u001b[0;32m    122\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mthread_timeout,\n\u001b[0;32m    123\u001b[0m     )\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m group_cm\u001b[38;5;241m.\u001b[39mended:\n",
      "File \u001b[1;32mc:\\app\\BaseToolkit\\lib\\asyncio\\tasks.py:492\u001b[0m, in \u001b[0;36mwait_for\u001b[1;34m(fut, timeout, loop)\u001b[0m\n\u001b[0;32m    490\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m fut\u001b[38;5;241m.\u001b[39mresult()\n\u001b[0;32m    491\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mCancelledError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m--> 492\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTimeoutError() \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    494\u001b[0m     timeout_handle\u001b[38;5;241m.\u001b[39mcancel()\n",
      "\u001b[1;31mTimeoutError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "result = evaluate(dataset, metrics=metrics, llm = langchain_llm, embeddings = langchain_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mresult\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## from webinar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attach to the existing event loop when using jupyter notebooks\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ArithmeticError\\ncwd = os.getcwd()\\nrelative_path = \\'/../../Evaluation/data/blog_eval_answers.csv\\'\\nfile_path = cwd + relative_path\\n# print(f\"file_path: {file_path}\")\\n\\n# Read ground truth answers from a CSV file.\\neval_df = pd.read_csv(file_path, header=0, skip_blank_lines=True)\\ndisplay(eval_df.head())\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, sys, pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ragas, datasets\n",
    "\n",
    "# Libraries to customize ragas critic model.\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "# Libraries to customize ragas embedding model.\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "\n",
    "# Import custom functions for evaluation.\n",
    "#sys.path.append(\"../../Evaluation\")\n",
    "#import eval_ragas as _eval_ragas\n",
    "\n",
    "# Import the evaluation metrics.\n",
    "from ragas.metrics import (\n",
    "    context_recall, \n",
    "    context_precision, \n",
    "    faithfulness, \n",
    "    answer_relevancy, \n",
    "    answer_similarity,\n",
    "    answer_correctness\n",
    "    )\n",
    "'''ArithmeticError\n",
    "cwd = os.getcwd()\n",
    "relative_path = '/../../Evaluation/data/blog_eval_answers.csv'\n",
    "file_path = cwd + relative_path\n",
    "# print(f\"file_path: {file_path}\")\n",
    "\n",
    "# Read ground truth answers from a CSV file.\n",
    "eval_df = pd.read_csv(file_path, header=0, skip_blank_lines=True)\n",
    "display(eval_df.head())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\app\\BaseToolkit\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset \n",
    "data_samples = {\n",
    "    'question': ['When was the first super bowl?', 'Who won the most super bowls?'],\n",
    "    'answer': ['The first superbowl was held on Jan 15, 1967', 'The most super bowls have been won by The New England Patriots'],\n",
    "    'contexts': [['The First AFL–NFL World Championship Game was an American football game played on January 15, 1967, at the Los Angeles Memorial Coliseum in Los Angeles,'],\n",
    "                 ['The Green Bay Packers...Green Bay, Wisconsin.', 'The Packers compete...Football Conference']],\n",
    "    'ground_truth': ['The first superbowl was held on January 15, 1967', 'The New England Patriots have won the Super Bowl a record six times']\n",
    "}\n",
    "\n",
    "\n",
    "dataset = Dataset.from_dict(data_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Set the evaluation type.\n",
    "EVALUATE_WHAT = 'ANSWERS' \n",
    "EVALUATE_WHAT = 'CONTEXTS'\n",
    "##########################################\n",
    "\n",
    "# Set the metrics to evaluate.\n",
    "if EVALUATE_WHAT == 'ANSWERS':\n",
    "    eval_metrics=[\n",
    "        answer_relevancy,\n",
    "        answer_similarity,\n",
    "        answer_correctness,\n",
    "        faithfulness,\n",
    "        ]\n",
    "    metrics = ['answer_relevancy', 'answer_similarity', 'answer_correctness', 'faithfulness']\n",
    "elif EVALUATE_WHAT == 'CONTEXTS':\n",
    "    eval_metrics=[\n",
    "        context_recall, \n",
    "        context_precision, \n",
    "        ]\n",
    "    metrics = ['context_recall', 'context_precision']\n",
    "    \n",
    "# Change the default llm-as-critic model to gpt-3.5-turbo.\n",
    "# LLM_NAME = \"gpt-3.5-turbo\" #OpenAI\n",
    "# ragas_llm = ragas.llms.llm_factory(model=LLM_NAME)\n",
    "\n",
    "# Change the default the llm-as-critic model to local llama3.\n",
    "LLM_NAME = 'llama3' #'llama3.1'\n",
    "ragas_llm = LangchainLLMWrapper(langchain_llm=ChatOllama(model=LLM_NAME))\n",
    "\n",
    "# Change the default embeddings models to use model on HuggingFace.\n",
    "EMB_NAME = \"BAAI/bge-large-en-v1.5\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "lc_embed_model = HuggingFaceEmbeddings(\n",
    "    model_name=EMB_NAME,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "ragas_emb = LangchainEmbeddingsWrapper(embeddings=lc_embed_model)\n",
    "\n",
    "# Change embeddings and critic models for each metric.\n",
    "for metric in metrics:\n",
    "    globals()[metric].llm = ragas_llm\n",
    "    globals()[metric].embeddings = ragas_emb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Define function to evaluate RAGAS model.\n",
    "def evaluate_ragas_model(data_samples, \n",
    "                         ragas_eval_metrics, \n",
    "                         what_to_evaluate='CONTEXTS'):\n",
    "    \"\"\"Evaluate the RAGAS model using the input pandas df.\"\"\"\n",
    "\n",
    "    #temp_df = pandas_eval_df.copy()\n",
    "    ragas_results_df_list = []\n",
    "    #scores = []\n",
    "    # Set the metrics to evaluate.\n",
    "    if EVALUATE_WHAT == 'ANSWERS':\n",
    "        eval_metrics=[\n",
    "            answer_relevancy,\n",
    "            answer_similarity,\n",
    "            answer_correctness,\n",
    "            faithfulness,\n",
    "            ]\n",
    "        #metrics = ['answer_relevancy', 'answer_similarity', 'answer_correctness', 'faithfulness']\n",
    "    elif EVALUATE_WHAT == 'CONTEXTS':\n",
    "            eval_metrics=[\n",
    "            context_recall, \n",
    "            #context_precision,\n",
    "            ]\n",
    "        #metrics = ['context_recall', 'context_precision']\n",
    "\n",
    "    ragas_results = ragas.evaluate(data_samples, metrics=eval_metrics)\n",
    "    print(ragas_results)\n",
    "\n",
    "    '''\n",
    "    # Return evaluations as pandas df.\n",
    "    temp = ragas_results.to_pandas()\n",
    "\n",
    "    temp_score = -1.0\n",
    "    if what_to_evaluate == \"CONTEXTS\":\n",
    "        # Calculate context F1 scores.\n",
    "        temp['context_f1'] = \\\n",
    "                2.0 * temp.context_precision * temp.context_recall \\\n",
    "                / (temp.context_precision + temp.context_recall)\n",
    "        temp = temp.fillna(0.0)\n",
    "        # Calculate Retrieval average score.\n",
    "        avg_retrieval_f1 = np.round(temp.context_f1.mean(),2)\n",
    "        temp_score = avg_retrieval_f1\n",
    "\n",
    "    elif what_to_evaluate == \"ANSWERS\":\n",
    "        # Calculate avg LLM answer scores across all floating point number scores between 0 and 1.\n",
    "        # temp['avg_answer_score'] = (temp.answer_relevancy + temp.answer_similarity + temp.answer_correctness) / 3\n",
    "        temp['avg_answer_score'] = temp.answer_correctness\n",
    "        avg_answer_score = np.round(temp.avg_answer_score.mean(),4)\n",
    "        temp_score = avg_answer_score\n",
    "    print(f\"avg_score: {temp_score}\")\n",
    "\n",
    "\n",
    "    # Append temp to the list of results.\n",
    "    ragas_results_df_list.append(temp)\n",
    "        \n",
    "    \n",
    "\n",
    "    # Return concantenated results and scores.\n",
    "    ragas_results_df = pd.concat(ragas_results_df_list, ignore_index=True)\n",
    "    return ragas_results_df_list\n",
    "    '''\n",
    "    return ragas_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating CONTEXTS using eval questions:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/2 [01:20<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TimeoutError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\app\\BaseToolkit\\lib\\asyncio\\tasks.py:258\u001b[0m, in \u001b[0;36mTask.__step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    257\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 258\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\app\\BaseToolkit\\lib\\site-packages\\ragas\\metrics\\_context_recall.py:168\u001b[0m, in \u001b[0;36mContextRecall._ascore\u001b[1;34m(self, row, callbacks)\u001b[0m\n\u001b[0;32m    167\u001b[0m p_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_context_recall_prompt(row)\n\u001b[1;32m--> 168\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m    169\u001b[0m     p_value,\n\u001b[0;32m    170\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m    171\u001b[0m     n\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreproducibility,\n\u001b[0;32m    172\u001b[0m )\n\u001b[0;32m    173\u001b[0m results \u001b[38;5;241m=\u001b[39m [results\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][i]\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreproducibility)]\n",
      "File \u001b[1;32mc:\\app\\BaseToolkit\\lib\\site-packages\\ragas\\llms\\base.py:95\u001b[0m, in \u001b[0;36mBaseRagasLLM.generate\u001b[1;34m(self, prompt, n, temperature, stop, callbacks, is_async)\u001b[0m\n\u001b[0;32m     92\u001b[0m     agenerate_text_with_retry \u001b[38;5;241m=\u001b[39m add_async_retry(\n\u001b[0;32m     93\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magenerate_text, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_config\n\u001b[0;32m     94\u001b[0m     )\n\u001b[1;32m---> 95\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m agenerate_text_with_retry(\n\u001b[0;32m     96\u001b[0m         prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[0;32m     97\u001b[0m         n\u001b[38;5;241m=\u001b[39mn,\n\u001b[0;32m     98\u001b[0m         temperature\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[0;32m     99\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    100\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m    101\u001b[0m     )\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\app\\BaseToolkit\\lib\\site-packages\\tenacity\\_asyncio.py:88\u001b[0m, in \u001b[0;36mAsyncRetrying.wraps.<locals>.async_wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21masync_wrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs: t\u001b[38;5;241m.\u001b[39mAny, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: t\u001b[38;5;241m.\u001b[39mAny) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mAny:\n\u001b[1;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\app\\BaseToolkit\\lib\\site-packages\\tenacity\\_asyncio.py:57\u001b[0m, in \u001b[0;36mAsyncRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     56\u001b[0m     retry_state\u001b[38;5;241m.\u001b[39mprepare_for_next_attempt()\n\u001b[1;32m---> 57\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msleep(do)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\app\\BaseToolkit\\lib\\asyncio\\tasks.py:652\u001b[0m, in \u001b[0;36msleep\u001b[1;34m(delay, result, loop)\u001b[0m\n\u001b[0;32m    651\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 652\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m future\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\app\\BaseToolkit\\lib\\asyncio\\futures.py:284\u001b[0m, in \u001b[0;36mFuture.__await__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    283\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_asyncio_future_blocking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 284\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m  \u001b[38;5;66;03m# This tells Task to wait for completion.\u001b[39;00m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[1;32mc:\\app\\BaseToolkit\\lib\\asyncio\\tasks.py:328\u001b[0m, in \u001b[0;36mTask.__wakeup\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 328\u001b[0m     \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\app\\BaseToolkit\\lib\\asyncio\\futures.py:196\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    195\u001b[0m     exc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_cancelled_error()\n\u001b[1;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m!=\u001b[39m _FINISHED:\n",
      "\u001b[1;31mCancelledError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\app\\BaseToolkit\\lib\\asyncio\\tasks.py:490\u001b[0m, in \u001b[0;36mwait_for\u001b[1;34m(fut, timeout, loop)\u001b[0m\n\u001b[0;32m    489\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 490\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mCancelledError \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\app\\BaseToolkit\\lib\\asyncio\\futures.py:196\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    195\u001b[0m     exc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_cancelled_error()\n\u001b[1;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m!=\u001b[39m _FINISHED:\n",
      "\u001b[1;31mCancelledError\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Execute the evaluation.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEVALUATE_WHAT\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m using eval questions:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m ragas_result\u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_ragas_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_metrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwhat_to_evaluate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEVALUATE_WHAT\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 26\u001b[0m, in \u001b[0;36mevaluate_ragas_model\u001b[1;34m(data_samples, ragas_eval_metrics, what_to_evaluate)\u001b[0m\n\u001b[0;32m     20\u001b[0m         eval_metrics\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m     21\u001b[0m         context_recall, \n\u001b[0;32m     22\u001b[0m         \u001b[38;5;66;03m#context_precision,\u001b[39;00m\n\u001b[0;32m     23\u001b[0m         ]\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;66;03m#metrics = ['context_recall', 'context_precision']\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m ragas_results \u001b[38;5;241m=\u001b[39m \u001b[43mragas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(ragas_results)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;124;03m# Return evaluations as pandas df.\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;124;03mtemp = ragas_results.to_pandas()\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03mreturn ragas_results_df_list\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n",
      "File \u001b[1;32mc:\\app\\BaseToolkit\\lib\\site-packages\\ragas\\evaluation.py:247\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(dataset, metrics, llm, embeddings, callbacks, in_ci, run_config, raise_exceptions, column_map)\u001b[0m\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evaluation_group_cm\u001b[38;5;241m.\u001b[39mended:\n\u001b[0;32m    245\u001b[0m         evaluation_rm\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 247\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    249\u001b[0m     result \u001b[38;5;241m=\u001b[39m Result(\n\u001b[0;32m    250\u001b[0m         scores\u001b[38;5;241m=\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_list(scores),\n\u001b[0;32m    251\u001b[0m         dataset\u001b[38;5;241m=\u001b[39mdataset,\n\u001b[0;32m    252\u001b[0m         binary_columns\u001b[38;5;241m=\u001b[39mbinary_metrics,\n\u001b[0;32m    253\u001b[0m     )\n",
      "File \u001b[1;32mc:\\app\\BaseToolkit\\lib\\site-packages\\ragas\\evaluation.py:227\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(dataset, metrics, llm, embeddings, callbacks, in_ci, run_config, raise_exceptions, column_map)\u001b[0m\n\u001b[0;32m    224\u001b[0m scores \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    225\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    226\u001b[0m     \u001b[38;5;66;03m# get the results\u001b[39;00m\n\u001b[1;32m--> 227\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mexecutor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresults\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m results \u001b[38;5;241m==\u001b[39m []:\n\u001b[0;32m    229\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ExceptionInRunner()\n",
      "File \u001b[1;32mc:\\app\\BaseToolkit\\lib\\site-packages\\ragas\\executor.py:107\u001b[0m, in \u001b[0;36mExecutor.results\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    103\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(r)\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 107\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_aresults\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m sorted_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(results, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [r[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m sorted_results]\n",
      "File \u001b[1;32mc:\\app\\BaseToolkit\\lib\\site-packages\\nest_asyncio.py:30\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[1;34m(main, debug)\u001b[0m\n\u001b[0;32m     28\u001b[0m task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(main)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[1;32mc:\\app\\BaseToolkit\\lib\\site-packages\\nest_asyncio.py:98\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m     97\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent loop stopped before Future completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\app\\BaseToolkit\\lib\\asyncio\\futures.py:201\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[1;32mc:\\app\\BaseToolkit\\lib\\asyncio\\tasks.py:256\u001b[0m, in \u001b[0;36mTask.__step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    253\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[0;32m    255\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[1;32m--> 256\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    257\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    258\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "File \u001b[1;32mc:\\app\\BaseToolkit\\lib\\site-packages\\ragas\\executor.py:102\u001b[0m, in \u001b[0;36mExecutor.results.<locals>._aresults\u001b[1;34m()\u001b[0m\n\u001b[0;32m     94\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m tqdm(\n\u001b[0;32m     96\u001b[0m     futures_as_they_finish,\n\u001b[0;32m     97\u001b[0m     desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdesc,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    100\u001b[0m     leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_progress_bar,\n\u001b[0;32m    101\u001b[0m ):\n\u001b[1;32m--> 102\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m future\n\u001b[0;32m    103\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(r)\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[1;32mc:\\app\\BaseToolkit\\lib\\asyncio\\tasks.py:611\u001b[0m, in \u001b[0;36mas_completed.<locals>._wait_for_one\u001b[1;34m()\u001b[0m\n\u001b[0;32m    608\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    609\u001b[0m     \u001b[38;5;66;03m# Dummy value from _on_timeout().\u001b[39;00m\n\u001b[0;32m    610\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTimeoutError\n\u001b[1;32m--> 611\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\app\\BaseToolkit\\lib\\asyncio\\futures.py:201\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[1;32mc:\\app\\BaseToolkit\\lib\\asyncio\\tasks.py:256\u001b[0m, in \u001b[0;36mTask.__step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    253\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[0;32m    255\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[1;32m--> 256\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    257\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    258\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "File \u001b[1;32mc:\\app\\BaseToolkit\\lib\\site-packages\\ragas\\executor.py:34\u001b[0m, in \u001b[0;36mas_completed.<locals>.sema_coro\u001b[1;34m(coro)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msema_coro\u001b[39m(coro):\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m semaphore:\n\u001b[1;32m---> 34\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m coro\n",
      "File \u001b[1;32mc:\\app\\BaseToolkit\\lib\\site-packages\\ragas\\executor.py:59\u001b[0m, in \u001b[0;36mExecutor.wrap_callable_with_index.<locals>.wrapped_callable_async\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraise_exceptions:\n\u001b[1;32m---> 59\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     61\u001b[0m         logger\u001b[38;5;241m.\u001b[39merror(\n\u001b[0;32m     62\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunner in Executor raised an exception\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     63\u001b[0m         )\n",
      "File \u001b[1;32mc:\\app\\BaseToolkit\\lib\\site-packages\\ragas\\executor.py:53\u001b[0m, in \u001b[0;36mExecutor.wrap_callable_with_index.<locals>.wrapped_callable_async\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     51\u001b[0m result \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnan\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 53\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m MaxRetriesExceeded \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;66;03m# this only for testset generation v2\u001b[39;00m\n\u001b[0;32m     56\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax retries exceeded for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39mevolution\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\app\\BaseToolkit\\lib\\site-packages\\ragas\\metrics\\base.py:127\u001b[0m, in \u001b[0;36mMetric.ascore\u001b[1;34m(self, row, callbacks, thread_timeout)\u001b[0m\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m group_cm\u001b[38;5;241m.\u001b[39mended:\n\u001b[0;32m    126\u001b[0m         rm\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 127\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m group_cm\u001b[38;5;241m.\u001b[39mended:\n",
      "File \u001b[1;32mc:\\app\\BaseToolkit\\lib\\site-packages\\ragas\\metrics\\base.py:120\u001b[0m, in \u001b[0;36mMetric.ascore\u001b[1;34m(self, row, callbacks, thread_timeout)\u001b[0m\n\u001b[0;32m    118\u001b[0m rm, group_cm \u001b[38;5;241m=\u001b[39m new_group(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, inputs\u001b[38;5;241m=\u001b[39mrow, callbacks\u001b[38;5;241m=\u001b[39mcallbacks)\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 120\u001b[0m     score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mwait_for(\n\u001b[0;32m    121\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ascore(row\u001b[38;5;241m=\u001b[39mrow, callbacks\u001b[38;5;241m=\u001b[39mgroup_cm),\n\u001b[0;32m    122\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mthread_timeout,\n\u001b[0;32m    123\u001b[0m     )\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m group_cm\u001b[38;5;241m.\u001b[39mended:\n",
      "File \u001b[1;32mc:\\app\\BaseToolkit\\lib\\asyncio\\tasks.py:492\u001b[0m, in \u001b[0;36mwait_for\u001b[1;34m(fut, timeout, loop)\u001b[0m\n\u001b[0;32m    490\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m fut\u001b[38;5;241m.\u001b[39mresult()\n\u001b[0;32m    491\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mCancelledError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m--> 492\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTimeoutError() \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    494\u001b[0m     timeout_handle\u001b[38;5;241m.\u001b[39mcancel()\n",
      "\u001b[1;31mTimeoutError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Execute the evaluation.\n",
    "print(f\"Evaluating {EVALUATE_WHAT} using eval questions:\")\n",
    "ragas_result= evaluate_ragas_model(\n",
    "    dataset, \n",
    "    eval_metrics, \n",
    "    what_to_evaluate=EVALUATE_WHAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context_recall': nan}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ragas_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loop = asyncio.get_running_loop()\n",
    "await loop.run_in_executor(None, self.generate, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating CONTEXTS using eval questions:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/4 [02:09<?, ?it/s]\n",
      "Exception in thread Thread-6:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\site-packages\\aiohttp\\connector.py\", line 1025, in _wrap_create_connection\n",
      "    return await self._loop.create_connection(*args, **kwargs)\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\asyncio\\base_events.py\", line 1065, in create_connection\n",
      "    raise exceptions[0]\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\asyncio\\base_events.py\", line 1050, in create_connection\n",
      "    sock = await self._connect_sock(\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\asyncio\\base_events.py\", line 961, in _connect_sock\n",
      "    await self.sock_connect(sock, address)\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\asyncio\\selector_events.py\", line 500, in sock_connect\n",
      "    return await fut\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\asyncio\\futures.py\", line 284, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\asyncio\\tasks.py\", line 328, in __wakeup\n",
      "    future.result()\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\asyncio\\futures.py\", line 201, in result\n",
      "    raise self._exception\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\asyncio\\selector_events.py\", line 535, in _sock_connect_cb\n",
      "    raise OSError(err, f'Connect call failed {address}')\n",
      "ConnectionRefusedError: [Errno 10061] Connect call failed ('127.0.0.1', 11434)\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\threading.py\", line 980, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\site-packages\\ragas\\executor.py\", line 95, in run\n",
      "    results = self.loop.run_until_complete(self._aresults())\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\asyncio\\futures.py\", line 201, in result\n",
      "    raise self._exception\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\asyncio\\tasks.py\", line 256, in __step\n",
      "    result = coro.send(None)\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\site-packages\\ragas\\executor.py\", line 83, in _aresults\n",
      "    raise e\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\site-packages\\ragas\\executor.py\", line 78, in _aresults\n",
      "    r = await future\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\asyncio\\tasks.py\", line 611, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\asyncio\\futures.py\", line 201, in result\n",
      "    raise self._exception\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\asyncio\\tasks.py\", line 256, in __step\n",
      "    result = coro.send(None)\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\site-packages\\ragas\\executor.py\", line 37, in sema_coro\n",
      "    return await coro\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\site-packages\\ragas\\executor.py\", line 111, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\site-packages\\ragas\\metrics\\base.py\", line 125, in ascore\n",
      "    raise e\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\site-packages\\ragas\\metrics\\base.py\", line 121, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\site-packages\\ragas\\metrics\\_context_recall.py\", line 169, in _ascore\n",
      "    results = await self.llm.generate(\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\site-packages\\ragas\\llms\\base.py\", line 93, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\site-packages\\tenacity\\_asyncio.py\", line 88, in async_wrapped\n",
      "    return await fn(*args, **kwargs)\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\site-packages\\tenacity\\_asyncio.py\", line 47, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\site-packages\\tenacity\\__init__.py\", line 325, in iter\n",
      "    raise retry_exc.reraise()\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\site-packages\\tenacity\\__init__.py\", line 158, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\concurrent\\futures\\_base.py\", line 439, in result\n",
      "    return self.__get_result()\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\concurrent\\futures\\_base.py\", line 391, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\site-packages\\tenacity\\_asyncio.py\", line 50, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\site-packages\\ragas\\llms\\base.py\", line 178, in agenerate_text\n",
      "    result = await self.langchain_llm.agenerate_prompt(\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 691, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 651, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\asyncio\\tasks.py\", line 256, in __step\n",
      "    result = coro.send(None)\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 836, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 323, in _agenerate\n",
      "    final_chunk = await self._achat_stream_with_aggregation(\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 244, in _achat_stream_with_aggregation\n",
      "    async for stream_resp in self._acreate_chat_stream(messages, stop, **kwargs):\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 203, in _acreate_chat_stream\n",
      "    async for stream_resp in self._acreate_stream(\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\site-packages\\langchain_community\\llms\\ollama.py\", line 295, in _acreate_stream\n",
      "    async with session.post(\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\site-packages\\aiohttp\\client.py\", line 1197, in __aenter__\n",
      "    self._resp = await self._coro\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\site-packages\\aiohttp\\client.py\", line 581, in _request\n",
      "    conn = await self._connector.connect(\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\site-packages\\aiohttp\\connector.py\", line 544, in connect\n",
      "    proto = await self._create_connection(req, traces, timeout)\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\site-packages\\aiohttp\\connector.py\", line 944, in _create_connection\n",
      "    _, proto = await self._create_direct_connection(req, traces, timeout)\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\site-packages\\aiohttp\\connector.py\", line 1257, in _create_direct_connection\n",
      "    raise last_exc\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\site-packages\\aiohttp\\connector.py\", line 1226, in _create_direct_connection\n",
      "    transp, proto = await self._wrap_create_connection(\n",
      "  File \"c:\\app\\BaseToolkit\\lib\\site-packages\\aiohttp\\connector.py\", line 1033, in _wrap_create_connection\n",
      "    raise client_error(req.connection_key, exc) from exc\n",
      "aiohttp.client_exceptions.ClientConnectorError: Cannot connect to host localhost:11434 ssl:default [Connect call failed ('127.0.0.1', 11434)]\n"
     ]
    },
    {
     "ename": "ExceptionInRunner",
     "evalue": "The runner thread which was running the jobs raised an exeception. Read the traceback above to debug it. You can also pass `raise_exceptions=False` incase you want to show only a warning message instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mExceptionInRunner\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Execute the evaluation.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEVALUATE_WHAT\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m using eval questions:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m ragas_result\u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_ragas_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_metrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwhat_to_evaluate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEVALUATE_WHAT\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 26\u001b[0m, in \u001b[0;36mevaluate_ragas_model\u001b[1;34m(data_samples, ragas_eval_metrics, what_to_evaluate)\u001b[0m\n\u001b[0;32m     20\u001b[0m         eval_metrics\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m     21\u001b[0m         context_recall, \n\u001b[0;32m     22\u001b[0m         context_precision,\n\u001b[0;32m     23\u001b[0m         ]\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;66;03m#metrics = ['context_recall', 'context_precision']\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m ragas_results \u001b[38;5;241m=\u001b[39m \u001b[43mragas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(ragas_results)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Return evaluations as pandas df.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\app\\BaseToolkit\\lib\\site-packages\\ragas\\evaluation.py:250\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(dataset, metrics, llm, embeddings, callbacks, in_ci, is_async, run_config, raise_exceptions, column_map)\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evaluation_group_cm\u001b[38;5;241m.\u001b[39mended:\n\u001b[0;32m    248\u001b[0m         evaluation_rm\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 250\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    252\u001b[0m     result \u001b[38;5;241m=\u001b[39m Result(\n\u001b[0;32m    253\u001b[0m         scores\u001b[38;5;241m=\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_list(scores),\n\u001b[0;32m    254\u001b[0m         dataset\u001b[38;5;241m=\u001b[39mdataset,\n\u001b[0;32m    255\u001b[0m         binary_columns\u001b[38;5;241m=\u001b[39mbinary_metrics,\n\u001b[0;32m    256\u001b[0m     )\n",
      "File \u001b[1;32mc:\\app\\BaseToolkit\\lib\\site-packages\\ragas\\evaluation.py:232\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(dataset, metrics, llm, embeddings, callbacks, in_ci, is_async, run_config, raise_exceptions, column_map)\u001b[0m\n\u001b[0;32m    230\u001b[0m results \u001b[38;5;241m=\u001b[39m executor\u001b[38;5;241m.\u001b[39mresults()\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m results \u001b[38;5;241m==\u001b[39m []:\n\u001b[1;32m--> 232\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ExceptionInRunner()\n\u001b[0;32m    234\u001b[0m \u001b[38;5;66;03m# convert results to dataset_like\u001b[39;00m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataset):\n",
      "\u001b[1;31mExceptionInRunner\u001b[0m: The runner thread which was running the jobs raised an exeception. Read the traceback above to debug it. You can also pass `raise_exceptions=False` incase you want to show only a warning message instead."
     ]
    }
   ],
   "source": [
    "# Execute the evaluation.\n",
    "print(f\"Evaluating {EVALUATE_WHAT} using eval questions:\")\n",
    "ragas_result= evaluate_ragas_model(\n",
    "    dataset, \n",
    "    eval_metrics, \n",
    "    what_to_evaluate=EVALUATE_WHAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ragas_result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mragas_result\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ragas_result' is not defined"
     ]
    }
   ],
   "source": [
    "ragas_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# from new documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using DeepEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\app\\BaseToolkit\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'deepeval'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeepeval\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DeepEvalBaseLLM\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mMistral7B\u001b[39;00m(DeepEvalBaseLLM):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m      7\u001b[0m         model,\n\u001b[0;32m      8\u001b[0m         tokenizer\n\u001b[0;32m      9\u001b[0m     ):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'deepeval'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from deepeval.models.base_model import DeepEvalBaseLLM\n",
    "\n",
    "class Mistral7B(DeepEvalBaseLLM):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        tokenizer\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def load_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        model = self.load_model()\n",
    "\n",
    "        device = \"cpu\" # \"cuda\" # the device to load the model onto\n",
    "\n",
    "        model_inputs = self.tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
    "        model.to(device)\n",
    "\n",
    "        generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)\n",
    "        return self.tokenizer.batch_decode(generated_ids)[0]\n",
    "\n",
    "    async def a_generate(self, prompt: str) -> str:\n",
    "        return self.generate(prompt)\n",
    "\n",
    "    def get_model_name(self):\n",
    "        return \"Mistral 7B\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "\n",
    "mistral_7b = Mistral7B(model=model, tokenizer=tokenizer)\n",
    "#print(mistral_7b.generate(\"Write me a joke\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import AnswerRelevancyMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "# Replace this with the actual output from your LLM application\n",
    "actual_output = \"We offer a 30-day full refund at no extra cost.\"\n",
    "\n",
    "metric = AnswerRelevancyMetric(\n",
    "    threshold=0.7,\n",
    "    model=\"mistral_7b\",\n",
    "    include_reason=True\n",
    ")\n",
    "test_case = LLMTestCase(\n",
    "    input=\"What if these shoes don't fit?\",\n",
    "    actual_output=actual_output\n",
    ")\n",
    "\n",
    "metric.measure(test_case)\n",
    "print(metric.score)\n",
    "print(metric.reason)\n",
    "\n",
    "# or evaluate test cases in bulk\n",
    "evaluate([test_case], [metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "test_case = LLMTestCase(\n",
    "    input=\"What if these shoes don't fit?\",\n",
    "    expected_output=\"You're eligible for a 30 day refund at no extra cost.\",\n",
    "    actual_output=\"We offer a 30-day full refund at no extra cost.\",\n",
    "    context=[\"All customers are eligible for a 30 day full refund at no extra cost.\"],\n",
    "    retrieval_context=[\"Only shoes can be refunded.\"],\n",
    "    tools_used=[\"WebSearch\"],\n",
    "    expected_tools=[\"WebSearch\", \"QueryDatabase\"]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
