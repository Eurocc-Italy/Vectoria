{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5c3a1b8-5681-413b-b224-47acaf317611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attach to the existing event loop when using jupyter notebooks\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain_huggingface import HuggingFacePipeline, HuggingFaceEmbeddings\n",
    "\n",
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")\n",
    "from ragas import evaluate\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from datasets import Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c836c4b1-01ff-4355-af1b-86049e2414a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f474a814d444acb83f9355782f52624",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9502750a8fac4413b5d754400391496e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   1%|          | 41.9M/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff6e6d7c5198456abac55834eee47ddc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caa85c918a86414eb465171a00b263a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bec02f1e93e4f7f954b57ae00f9d971",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "# Define the LLM model and tokenizer\n",
    "#model_name = \"swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA\"\n",
    "model_name = \"sapienzanlp/Minerva-3B-base-v1.0\"\n",
    "model_name = \"microsoft/Phi-3-mini-128k-instruct\"\n",
    "#model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "# Create the pipeline\n",
    "#pipe = pipeline(\n",
    "##    model=model,\n",
    "#    tokenizer=tokenizer,\n",
    "#    return_full_text=True,\n",
    "#    task='text-generation',\n",
    "#    temperature=0.1, \n",
    "#    repetition_penalty=1.1\n",
    "#)\n",
    "pipe = pipeline(\"text-generation\", model=model_name, trust_remote_code=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "098bd80f-a721-4e57-8aaa-d974e5caf700",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the HuggingFacePipeline for LangChain\n",
    "langchain_llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# Embedding model\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d1ab16-62e2-4fa1-83e2-38fe702acd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Wrap the LLM and embeddings with the RAGAS wrappers\n",
    "langchain_llm = LangchainLLMWrapper(langchain_llm)\n",
    "langchain_embeddings = LangchainEmbeddingsWrapper(embedding_model)\n",
    "\n",
    "# Example data samples\n",
    "data_samples = {\n",
    "    'question': ['When was the first super bowl?', 'Who won the most super bowls?'],\n",
    "    'answer': ['The first superbowl was held on Jan 15, 1967', 'The most super bowls have been won by The New England Patriots'],\n",
    "    'contexts': [['The First AFLâ€“NFL World Championship Game was an American football game played on January 15, 1967, at the Los Angeles Memorial Coliseum in Los Angeles,'],\n",
    "                 ['The Green Bay Packers...Green Bay, Wisconsin.', 'The Packers compete...Football Conference']],\n",
    "    'ground_truth': ['The first superbowl was held on January 15, 1967', 'The New England Patriots have won the Super Bowl a record six times']\n",
    "}\n",
    "\n",
    "# Create a dataset from the data samples\n",
    "dataset = Dataset.from_dict(data_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d9ae82-1fb2-49b5-9e3a-825fab949807",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run evaluation\n",
    "result = evaluate(\n",
    "    dataset=dataset,\n",
    "    llm=langchain_llm,\n",
    "    embeddings=langchain_embeddings,\n",
    "    metrics=[\n",
    "        context_precision,\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "        context_recall,\n",
    "    ],\n",
    "    raise_exceptions=True, \n",
    "    is_async = True  # Ensure exceptions are raised directly\n",
    ")\n",
    "\n",
    "# Convert the evaluation result to a pandas DataFrame and print it\n",
    "evaluation_df = result.to_pandas()\n",
    "print(\"Evaluation Results:\\n\", evaluation_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
