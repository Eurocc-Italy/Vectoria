(0, Document(metadata={'layout_tag': 'Paragraph', 'doc_file_name': 'test_docx.docx', 'paragraph_number': '7.2', 'paragraph_name': 'Model Variations', 'seq_id': 0}, page_content='Table 3:Variations on the Transformer architecture. Unlisted values are identical to those of the base model. All metrics are on the English-to-German translation development set, newstest2013. Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities.'))
(1, Document(metadata={'layout_tag': 'Paragraph', 'doc_file_name': 'test_docx.docx', 'paragraph_number': '7.2', 'paragraph_name': 'Model Variations', 'seq_id': 1}, page_content='N dmodel dff h dk dv Pd\u2062r\u2062o\u2062p ϵl\u2062s train PPL BLEU params \n N dmodel dff h dk dv Pd\u2062r\u2062o\u2062p ϵl\u2062s steps (dev) (dev) ×106 \n\xa0base 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65 \n\xa0(A)\xa0    1 512 512    5.29 24.9  \n\xa0(A)\xa0    4 128 128    5.00 25.5  \n\xa0(A)\xa0    16 32 32    4.91 25.8  \n\xa0(A)\xa0    32 16 16    5.01 25.4  \n\xa0(B)\xa0     16     5.16 25.1 58 \n\xa0(B)\xa0     32     5.01 25.4 60 \n\xa0(C)\xa0 2         6.11 23.7 36 \n\xa0(C)\xa0 4         5.19 25.3 50 \n\xa0(C)\xa0 8         4.88 25.5 80 \n\xa0(C)\xa0  256   32 32    5.75 24.5 28'))
(2, Document(metadata={'layout_tag': 'Paragraph', 'doc_file_name': 'test_docx.docx', 'paragraph_number': '7.2', 'paragraph_name': 'Model Variations', 'seq_id': 2}, page_content='(C)\xa0 2         6.11 23.7 36 \n\xa0(C)\xa0 4         5.19 25.3 50 \n\xa0(C)\xa0 8         4.88 25.5 80 \n\xa0(C)\xa0  256   32 32    5.75 24.5 28 \n\xa0(C)\xa0  1024   128 128    4.66 26.0 168 \n\xa0(C)\xa0   1024       5.12 25.4 53 \n\xa0(C)\xa0   4096       4.75 26.2 90 \n\xa0(D)\xa0       0.0   5.77 24.6  \n\xa0(D)\xa0       0.2   4.95 25.5  \n\xa0(D)\xa0        0.0  4.67 25.3  \n\xa0(D)\xa0        0.2  5.47 25.7'))
(3, Document(metadata={'layout_tag': 'Paragraph', 'doc_file_name': 'test_docx.docx', 'paragraph_number': '7.2', 'paragraph_name': 'Model Variations', 'seq_id': 3}, page_content='(D)\xa0       0.0   5.77 24.6  \n\xa0(D)\xa0       0.2   4.95 25.5  \n\xa0(D)\xa0        0.0  4.67 25.3  \n\xa0(D)\xa0        0.2  5.47 25.7  \n\xa0(E)  positional embedding instead of sinusoids positional embedding instead of sinusoids positional embedding instead of sinusoids positional embedding instead of sinusoids positional embedding instead of sinusoids positional embedding instead of sinusoids positional embedding instead of sinusoids  4.92 25.7  \n\xa0big 6 1024 4096 16   0.3  300K 4.33 26.4 213'))
(4, Document(metadata={'layout_tag': 'Paragraph', 'doc_file_name': 'test_docx.docx', 'paragraph_number': '7.2', 'paragraph_name': 'Model Variations', 'seq_id': 4}, page_content='big 6 1024 4096 16   0.3  300K 4.33 26.4 213 \nTo evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the development set, newstest2013. We used beam search as described in the previous section, but no checkpoint averaging. We present these results in Table\xa03.'))
(5, Document(metadata={'layout_tag': 'Paragraph', 'doc_file_name': 'test_docx.docx', 'paragraph_number': '7.2', 'paragraph_name': 'Model Variations', 'seq_id': 5}, page_content='In Table\xa03\xa0rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section\xa03.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.'))
(6, Document(metadata={'layout_tag': 'Paragraph', 'doc_file_name': 'test_docx.docx', 'paragraph_number': '7.2', 'paragraph_name': 'Model Variations', 'seq_id': 6}, page_content='In Table\xa03\xa0rows (B), we observe that reducing the attention key size\xa0dk\xa0hurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our sinusoidal positional encoding with learned positional embeddings\xa0[9], and observe nearly identical results'))
(7, Document(metadata={'layout_tag': 'Paragraph', 'doc_file_name': 'test_docx.docx', 'paragraph_number': '7.2', 'paragraph_name': 'Model Variations', 'seq_id': 7}, page_content='(E) we replace our sinusoidal positional encoding with learned positional embeddings\xa0[9], and observe nearly identical results to the base model.'))
