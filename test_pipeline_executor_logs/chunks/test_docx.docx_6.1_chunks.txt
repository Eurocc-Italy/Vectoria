(0, Document(metadata={'layout_tag': 'Paragraph', 'doc_file_name': 'test_docx.docx', 'paragraph_number': '6.1', 'paragraph_name': 'Training Data and Batching', 'seq_id': 0}, page_content='We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding\xa0[3], which has a shared source-target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary\xa0[38]. Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of'))
(1, Document(metadata={'layout_tag': 'Paragraph', 'doc_file_name': 'test_docx.docx', 'paragraph_number': '6.1', 'paragraph_name': 'Training Data and Batching', 'seq_id': 1}, page_content='vocabulary\xa0[38]. Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.'))
