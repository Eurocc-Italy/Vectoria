(0, Document(metadata={'layout_tag': 'Paragraph', 'doc_file_name': 'test_docx.docx', 'paragraph_number': '4.2.3', 'paragraph_name': 'Applications of Attention in our Model', 'seq_id': 0}, page_content='The Transformer uses multi-head attention in three different ways:\nIn "encoder-decoder attention" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as\xa0[38,\xa02,\xa09].'))
(1, Document(metadata={'layout_tag': 'Paragraph', 'doc_file_name': 'test_docx.docx', 'paragraph_number': '4.2.3', 'paragraph_name': 'Applications of Attention in our Model', 'seq_id': 1}, page_content='The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.'))
(2, Document(metadata={'layout_tag': 'Paragraph', 'doc_file_name': 'test_docx.docx', 'paragraph_number': '4.2.3', 'paragraph_name': 'Applications of Attention in our Model', 'seq_id': 2}, page_content='Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to\xa0−∞) all values in the input of the softmax which correspond to illegal connections. See Figure\xa02.'))
