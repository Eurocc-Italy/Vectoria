(0, Document(metadata={'layout_tag': 'Paragraph', 'doc_file_name': 'test_docx.docx', 'paragraph_number': '5', 'paragraph_name': 'Why Self-Attention', 'seq_id': 0}, page_content='In this section we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations\xa0(x1,…,xn)\xa0to another sequence of equal length\xa0(z1,…,zn), with\xa0xi,zi∈ℝd, such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata.'))
(1, Document(metadata={'layout_tag': 'Paragraph', 'doc_file_name': 'test_docx.docx', 'paragraph_number': '5', 'paragraph_name': 'Why Self-Attention', 'seq_id': 1}, page_content='One is the total computational complexity per layer. Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required.'))
(2, Document(metadata={'layout_tag': 'Paragraph', 'doc_file_name': 'test_docx.docx', 'paragraph_number': '5', 'paragraph_name': 'Why Self-Attention', 'seq_id': 2}, page_content='The third is the path length between long-range dependencies in the network. Learning long-range dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network. The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies\xa0[12]. Hence we also compare the maximum path'))
(3, Document(metadata={'layout_tag': 'Paragraph', 'doc_file_name': 'test_docx.docx', 'paragraph_number': '5', 'paragraph_name': 'Why Self-Attention', 'seq_id': 3}, page_content='the input and output sequences, the easier it is to learn long-range dependencies\xa0[12]. Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types.'))
(4, Document(metadata={'layout_tag': 'Paragraph', 'doc_file_name': 'test_docx.docx', 'paragraph_number': '5', 'paragraph_name': 'Why Self-Attention', 'seq_id': 4}, page_content='Table 1:\xa0Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types.\xa0n\xa0is the sequence length,\xa0d\xa0is the representation dimension,\xa0k\xa0is the kernel size of convolutions and\xa0r\xa0the size of the neighborhood in restricted self-attention.'))
(5, Document(metadata={'layout_tag': 'Paragraph', 'doc_file_name': 'test_docx.docx', 'paragraph_number': '5', 'paragraph_name': 'Why Self-Attention', 'seq_id': 5}, page_content='Layer Type Complexity per Layer Sequential Maximum Path Length \n  Operations  \nSelf-Attention O\u2062(n2⋅d) O\u2062(1) O\u2062(1) \nRecurrent O\u2062(n⋅d2) O\u2062(n) O\u2062(n) \nConvolutional O\u2062(k⋅n⋅d2) O\u2062(1) O\u2062(l\u2062o\u2062gk\u2062(n)) \nSelf-Attention (restricted) O\u2062(r⋅n⋅d) O\u2062(1) O\u2062(n/r)'))
(6, Document(metadata={'layout_tag': 'Paragraph', 'doc_file_name': 'test_docx.docx', 'paragraph_number': '5', 'paragraph_name': 'Why Self-Attention', 'seq_id': 6}, page_content='As noted in Table\xa01, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires\xa0O\u2062(n)\xa0sequential operations. In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence length\xa0n\xa0is smaller than the representation dimensionality\xa0d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece\xa0[38]\xa0and'))
(7, Document(metadata={'layout_tag': 'Paragraph', 'doc_file_name': 'test_docx.docx', 'paragraph_number': '5', 'paragraph_name': 'Why Self-Attention', 'seq_id': 7}, page_content='the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece\xa0[38]\xa0and byte-pair\xa0[31]\xa0representations. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size\xa0r\xa0in the input sequence centered around the respective output position. This would increase the maximum path length to\xa0O\u2062(n/r). We plan to investigate this approach further in future work.'))
(8, Document(metadata={'layout_tag': 'Paragraph', 'doc_file_name': 'test_docx.docx', 'paragraph_number': '5', 'paragraph_name': 'Why Self-Attention', 'seq_id': 8}, page_content='A single convolutional layer with kernel width\xa0k<n\xa0does not connect all pairs of input and output positions. Doing so requires a stack of\xa0O\u2062(n/k)\xa0convolutional layers in the case of contiguous kernels, or\xa0O\u2062(l\u2062o\u2062gk\u2062(n))\xa0in the case of dilated convolutions\xa0[18], increasing the length of the longest paths between any two positions in the network. Convolutional layers are generally more expensive than recurrent layers, by a factor of\xa0k. Separable convolutions\xa0[6], however, decrease the complexity'))
(9, Document(metadata={'layout_tag': 'Paragraph', 'doc_file_name': 'test_docx.docx', 'paragraph_number': '5', 'paragraph_name': 'Why Self-Attention', 'seq_id': 9}, page_content='generally more expensive than recurrent layers, by a factor of\xa0k. Separable convolutions\xa0[6], however, decrease the complexity considerably, to\xa0O\u2062(k⋅n⋅d+n⋅d2). Even with\xa0k=n, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model.'))
(10, Document(metadata={'layout_tag': 'Paragraph', 'doc_file_name': 'test_docx.docx', 'paragraph_number': '5', 'paragraph_name': 'Why Self-Attention', 'seq_id': 10}, page_content='As side benefit, self-attention could yield more interpretable models. We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences.'))
