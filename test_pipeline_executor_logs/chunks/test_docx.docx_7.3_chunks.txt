(0, Document(metadata={'layout_tag': 'Paragraph', 'doc_file_name': 'test_docx.docx', 'paragraph_number': '7.3', 'paragraph_name': 'English Constituency Parsing', 'seq_id': 0}, page_content='Table 4:The Transformer generalizes well to English constituency parsing (Results are on Section 23 of WSJ)'))
(1, Document(metadata={'layout_tag': 'Paragraph', 'doc_file_name': 'test_docx.docx', 'paragraph_number': '7.3', 'paragraph_name': 'English Constituency Parsing', 'seq_id': 1}, page_content='Parser Training WSJ 23 F1 \nVinyals & Kaiser el al. (2014)\xa0[37]\xa0 WSJ only, discriminative 88.3 \nPetrov et al. (2006)\xa0[29]\xa0 WSJ only, discriminative 90.4 \nZhu et al. (2013)\xa0[40]\xa0 WSJ only, discriminative 90.4 \nDyer et al. (2016)\xa0[8]\xa0 WSJ only, discriminative 91.7 \nTransformer (4 layers) WSJ only, discriminative 91.3 \nZhu et al. (2013)\xa0[40]\xa0 semi-supervised 91.3 \nHuang & Harper (2009)\xa0[14]\xa0 semi-supervised 91.3 \nMcClosky et al. (2006)\xa0[26]\xa0 semi-supervised 92.1'))
(2, Document(metadata={'layout_tag': 'Paragraph', 'doc_file_name': 'test_docx.docx', 'paragraph_number': '7.3', 'paragraph_name': 'English Constituency Parsing', 'seq_id': 2}, page_content='Huang & Harper (2009)\xa0[14]\xa0 semi-supervised 91.3 \nMcClosky et al. (2006)\xa0[26]\xa0 semi-supervised 92.1 \nVinyals & Kaiser el al. (2014)\xa0[37]\xa0 semi-supervised 92.1 \nTransformer (4 layers) semi-supervised 92.7 \nLuong et al. (2015)\xa0[23]\xa0 multi-task 93.0 \nDyer et al. (2016)\xa0[8]\xa0 generative 93.3'))
(3, Document(metadata={'layout_tag': 'Paragraph', 'doc_file_name': 'test_docx.docx', 'paragraph_number': '7.3', 'paragraph_name': 'English Constituency Parsing', 'seq_id': 3}, page_content='Luong et al. (2015)\xa0[23]\xa0 multi-task 93.0 \nDyer et al. (2016)\xa0[8]\xa0 generative 93.3 \nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing. This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes\xa0[37].'))
(4, Document(metadata={'layout_tag': 'Paragraph', 'doc_file_name': 'test_docx.docx', 'paragraph_number': '7.3', 'paragraph_name': 'English Constituency Parsing', 'seq_id': 4}, page_content='We trained a 4-layer transformer with\xa0dm\u2062o\u2062d\u2062e\u2062l=1024\xa0on the Wall Street Journal (WSJ) portion of the Penn Treebank\xa0[25], about 40K training sentences. We also trained it in a semi-supervised setting, using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\xa0[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting.'))
(5, Document(metadata={'layout_tag': 'Paragraph', 'doc_file_name': 'test_docx.docx', 'paragraph_number': '7.3', 'paragraph_name': 'English Constituency Parsing', 'seq_id': 5}, page_content='We performed only a small number of experiments to select the dropout, both attention and residual (section\xa05.4), learning rates and beam size on the Section 22 development set, all other parameters remained unchanged from the English-to-German base translation model. During inference, we increased the maximum output length to input length +\xa0300. We used a beam size of\xa021\xa0and\xa0Î±=0.3\xa0for both WSJ only and the semi-supervised setting.'))
(6, Document(metadata={'layout_tag': 'Paragraph', 'doc_file_name': 'test_docx.docx', 'paragraph_number': '7.3', 'paragraph_name': 'English Constituency Parsing', 'seq_id': 6}, page_content='Our results in Table\xa04\xa0show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar\xa0[8].\nIn contrast to RNN sequence-to-sequence models\xa0[37], the Transformer outperforms the BerkeleyParser\xa0[29]\xa0even when training only on the WSJ training set of 40K sentences.'))
