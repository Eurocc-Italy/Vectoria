(0, Document(metadata={'layout_tag': 'Paragraph', 'doc_file_name': 'test_docx.docx', 'paragraph_number': '4.5', 'paragraph_name': 'Positional Encoding', 'seq_id': 0}, page_content='Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add "positional encodings" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension\xa0dmodel\xa0as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and'))
(1, Document(metadata={'layout_tag': 'Paragraph', 'doc_file_name': 'test_docx.docx', 'paragraph_number': '4.5', 'paragraph_name': 'Positional Encoding', 'seq_id': 1}, page_content='dimension\xa0dmodel\xa0as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed\xa0[9].'))
(2, Document(metadata={'layout_tag': 'Paragraph', 'doc_file_name': 'test_docx.docx', 'paragraph_number': '4.5', 'paragraph_name': 'Positional Encoding', 'seq_id': 2}, page_content='In this work, we use sine and cosine functions of different frequencies:'))
(3, Document(metadata={'layout_tag': 'Paragraph', 'doc_file_name': 'test_docx.docx', 'paragraph_number': '4.5', 'paragraph_name': 'Positional Encoding', 'seq_id': 3}, page_content='P\u2062E(p\u2062o\u2062s,2\u2062i)=s\u2062i\u2062n\u2062(p\u2062o\u2062s/100002\u2062i/dmodel)  \n P\u2062E(p\u2062o\u2062s,2\u2062i+1)=c\u2062o\u2062s\u2062(p\u2062o\u2062s/100002\u2062i/dmodel)  \nwhere\xa0p\u2062o\u2062s\xa0is the position and\xa0i\xa0is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from\xa02\u2062π\xa0to\xa010000⋅2\u2062π. We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset\xa0k,\xa0P\u2062Ep\u2062o\u2062s+k\xa0can be represented as a linear function of\xa0P\u2062Ep\u2062o\u2062s.'))
(4, Document(metadata={'layout_tag': 'Paragraph', 'doc_file_name': 'test_docx.docx', 'paragraph_number': '4.5', 'paragraph_name': 'Positional Encoding', 'seq_id': 4}, page_content='We also experimented with using learned positional embeddings\xa0[9]\xa0instead, and found that the two versions produced nearly identical results (see Table\xa03\xa0row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.'))
