(0, Document(metadata={'layout_tag': 'Paragraph', 'doc_file_name': 'test_docx.docx', 'paragraph_number': '4', 'paragraph_name': 'Model Architecture', 'seq_id': 0}, page_content='Figure 1:The Transformer - model architecture.'))
(1, Document(metadata={'layout_tag': 'Paragraph', 'doc_file_name': 'test_docx.docx', 'paragraph_number': '4', 'paragraph_name': 'Model Architecture', 'seq_id': 1}, page_content='Most competitive neural sequence transduction models have an encoder-decoder structure\xa0[5,\xa02,\xa035]. Here, the encoder maps an input sequence of symbol representations\xa0(x1,‚Ä¶,xn)\xa0to a sequence of continuous representations\xa0ùê≥=(z1,‚Ä¶,zn). Given\xa0ùê≥, the decoder then generates an output sequence\xa0(y1,‚Ä¶,ym)\xa0of symbols one element at a time. At each step the model is auto-regressive\xa0[10], consuming the previously generated symbols as additional input when generating the next.'))
(2, Document(metadata={'layout_tag': 'Paragraph', 'doc_file_name': 'test_docx.docx', 'paragraph_number': '4', 'paragraph_name': 'Model Architecture', 'seq_id': 2}, page_content='The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure\xa01, respectively.'))
