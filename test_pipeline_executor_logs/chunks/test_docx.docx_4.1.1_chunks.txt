(0, Document(metadata={'layout_tag': 'Paragraph', 'doc_file_name': 'test_docx.docx', 'paragraph_number': '4.1.1', 'paragraph_name': 'Encoder:', 'seq_id': 0}, page_content='The encoder is composed of a stack of\xa0N=6\xa0identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network. We employ a residual connection\xa0[11]\xa0around each of the two sub-layers, followed by layer normalization\xa0[1]. That is, the output of each sub-layer is\xa0LayerNorm\u2062(x+Sublayer\u2062(x)), where\xa0Sublayer\u2062(x)\xa0is the function implemented by the sub-layer itself. To facilitate these residual'))
(1, Document(metadata={'layout_tag': 'Paragraph', 'doc_file_name': 'test_docx.docx', 'paragraph_number': '4.1.1', 'paragraph_name': 'Encoder:', 'seq_id': 1}, page_content='where\xa0Sublayer\u2062(x)\xa0is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension\xa0dmodel=512.'))
