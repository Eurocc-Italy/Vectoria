(0, Document(metadata={'layout_tag': 'Paragraph', 'doc_file_name': 'test_docx.docx', 'paragraph_number': '6.3', 'paragraph_name': 'Optimizer', 'seq_id': 0}, page_content='We used the Adam optimizer\xa0[20]\xa0with\xa0β1=0.9,\xa0β2=0.98\xa0and\xa0ϵ=10−9. We varied the learning rate over the course of training, according to the formula:\n\n l\u2062r\u2062a\u2062t\u2062e=dmodel−0.5⋅min\u2061(s\u2062t\u2062e\u2062p\u2062_\u2062n\u2062u\u2062m−0.5,s\u2062t\u2062e\u2062p\u2062_\u2062n\u2062u\u2062m⋅w\u2062a\u2062r\u2062m\u2062u\u2062p\u2062_\u2062s\u2062t\u2062e\u2062p\u2062s−1.5)  (3) \nThis corresponds to increasing the learning rate linearly for the first\xa0w\u2062a\u2062r\u2062m\u2062u\u2062p\u2062_\u2062s\u2062t\u2062e\u2062p\u2062s\xa0training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used\xa0w\u2062a\u2062r\u2062m\u2062u\u2062p\u2062_\u2062s\u2062t\u2062e\u2062p\u2062s=4000.'))
