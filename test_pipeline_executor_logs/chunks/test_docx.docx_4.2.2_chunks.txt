(0, Document(metadata={'layout_tag': 'Paragraph', 'doc_file_name': 'test_docx.docx', 'paragraph_number': '4.2.2', 'paragraph_name': 'Multi-Head Attention', 'seq_id': 0}, page_content='Figure 2:(left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel.'))
(1, Document(metadata={'layout_tag': 'Paragraph', 'doc_file_name': 'test_docx.docx', 'paragraph_number': '4.2.2', 'paragraph_name': 'Multi-Head Attention', 'seq_id': 1}, page_content='Instead of performing a single attention function with\xa0dmodel-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values\xa0h\xa0times with different, learned linear projections to\xa0dk,\xa0dk\xa0and\xa0dv\xa0dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding\xa0dv-dimensional output values. These are concatenated and once again projected, resulting in the final values, as'))
(2, Document(metadata={'layout_tag': 'Paragraph', 'doc_file_name': 'test_docx.docx', 'paragraph_number': '4.2.2', 'paragraph_name': 'Multi-Head Attention', 'seq_id': 2}, page_content='yielding\xa0dv-dimensional output values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure\xa02.'))
(3, Document(metadata={'layout_tag': 'Paragraph', 'doc_file_name': 'test_docx.docx', 'paragraph_number': '4.2.2', 'paragraph_name': 'Multi-Head Attention', 'seq_id': 3}, page_content='Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.'))
(4, Document(metadata={'layout_tag': 'Paragraph', 'doc_file_name': 'test_docx.docx', 'paragraph_number': '4.2.2', 'paragraph_name': 'Multi-Head Attention', 'seq_id': 4}, page_content='MultiHead\u2062(Q,K,V) =Concat\u2062(head1,…,headh)\u2062WO  \n where\u2062headi =Attention\u2062(Q\u2062WiQ,K\u2062WiK,V\u2062WiV)  \nWhere the projections are parameter matrices\xa0WiQ∈ℝdmodel×dk,\xa0WiK∈ℝdmodel×dk,\xa0WiV∈ℝdmodel×dv\xa0and\xa0WO∈ℝh\u2062dv×dmodel.\nIn this work we employ\xa0h=8\xa0parallel attention layers, or heads. For each of these we use\xa0dk=dv=dmodel/h=64. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.'))
