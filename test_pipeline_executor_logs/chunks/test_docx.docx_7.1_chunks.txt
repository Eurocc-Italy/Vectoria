(0, Document(metadata={'layout_tag': 'Paragraph', 'doc_file_name': 'test_docx.docx', 'paragraph_number': '7.1', 'paragraph_name': 'Machine Translation', 'seq_id': 0}, page_content='Table 2:The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.'))
(1, Document(metadata={'layout_tag': 'Paragraph', 'doc_file_name': 'test_docx.docx', 'paragraph_number': '7.1', 'paragraph_name': 'Machine Translation', 'seq_id': 1}, page_content='Model BLEU BLEU  Training Cost (FLOPs) Training Cost (FLOPs) \nModel EN-DE EN-FR  EN-DE EN-FR \nByteNet\xa0[18]\xa0 23.75     \nDeep-Att + PosUnk\xa0[39]\xa0  39.2   1.0⋅1020 \nGNMT + RL\xa0[38]\xa0 24.6 39.92  2.3⋅1019 1.4⋅1020 \nConvS2S\xa0[9]\xa0 25.16 40.46  9.6⋅1018 1.5⋅1020 \nMoE\xa0[32]\xa0 26.03 40.56  2.0⋅1019 1.2⋅1020 \nDeep-Att + PosUnk Ensemble\xa0[39]\xa0  40.4   8.0⋅1020 \nGNMT + RL Ensemble\xa0[38]\xa0 26.30 41.16  1.8⋅1020 1.1⋅1021 \nConvS2S Ensemble\xa0[9]\xa0 26.36 41.29  7.7⋅1019 1.2⋅1021'))
(2, Document(metadata={'layout_tag': 'Paragraph', 'doc_file_name': 'test_docx.docx', 'paragraph_number': '7.1', 'paragraph_name': 'Machine Translation', 'seq_id': 2}, page_content='GNMT + RL Ensemble\xa0[38]\xa0 26.30 41.16  1.8⋅1020 1.1⋅1021 \nConvS2S Ensemble\xa0[9]\xa0 26.36 41.29  7.7⋅1019 1.2⋅1021 \nTransformer (base model) 27.3 38.1  3.3⋅𝟏𝟎𝟏𝟖 3.3⋅𝟏𝟎𝟏𝟖 \nTransformer (big) 28.4 41.8  2.3⋅1019 2.3⋅1019'))
(3, Document(metadata={'layout_tag': 'Paragraph', 'doc_file_name': 'test_docx.docx', 'paragraph_number': '7.1', 'paragraph_name': 'Machine Translation', 'seq_id': 3}, page_content='On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table\xa02) outperforms the best previously reported models (including ensembles) by more than\xa02.0\xa0BLEU, establishing a new state-of-the-art BLEU score of\xa028.4. The configuration of this model is listed in the bottom line of Table\xa03. Training took\xa03.5\xa0days on\xa08\xa0P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive'))
(4, Document(metadata={'layout_tag': 'Paragraph', 'doc_file_name': 'test_docx.docx', 'paragraph_number': '7.1', 'paragraph_name': 'Machine Translation', 'seq_id': 4}, page_content='model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models.'))
(5, Document(metadata={'layout_tag': 'Paragraph', 'doc_file_name': 'test_docx.docx', 'paragraph_number': '7.1', 'paragraph_name': 'Machine Translation', 'seq_id': 5}, page_content='On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of\xa041.0, outperforming all of the previously published single models, at less than\xa01/4\xa0the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate\xa0Pd\u2062r\u2062o\u2062p=0.1, instead of\xa00.3.'))
(6, Document(metadata={'layout_tag': 'Paragraph', 'doc_file_name': 'test_docx.docx', 'paragraph_number': '7.1', 'paragraph_name': 'Machine Translation', 'seq_id': 6}, page_content='For the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We used beam search with a beam size of\xa04\xa0and length penalty\xa0α=0.6\xa0[38]. These hyperparameters were chosen after experimentation on the development set. We set the maximum output length during inference to input length +\xa050, but terminate early when possible\xa0[38].'))
(7, Document(metadata={'layout_tag': 'Paragraph', 'doc_file_name': 'test_docx.docx', 'paragraph_number': '7.1', 'paragraph_name': 'Machine Translation', 'seq_id': 7}, page_content='Table\xa02\xa0summarizes our results and compares our translation quality and training costs to other model architectures from the literature. We estimate the number of floating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision floating-point capacity of each GPU\xa02.'))
